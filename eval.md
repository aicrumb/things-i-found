# Puplexity
---

The examination of a language model's (LM) perplexity in response to this and similar phrases serves as an indicator of the model's ability to generalize across diverse subcultures. By assessing the model's performance when confronted with unconventional linguistic patterns and emotive expressions, we can better comprehend the extent to which the LM has effectively captured the nuances and intricacies of various sociolects, thus shedding light on the model's adaptability and potential applicability in real-world linguistic contexts. This analysis was conducted utilizing the complimentary edition of Google Colab, thereby imposing certain limitations on the maximum model size that can be accommodated within the constraints of the computational resources provided by the platform.

| Model | Puplexity â†“ |
| --- | --- | 
| gpt2 | 491.57 |
| gpt2-medium | 123.56 |
| gpt2-large | 192.16 |
| gpt2-xl | 147.29 |
| facebook/opt-125m | 286.0 | 
| facebook/opt-350m | 202.87 |
| facebook/opt-1.3b | 101.18 |
| facebook/opt-2.7b | 96.56 |
| facebook/galactica-125m | 308.0 |
| facebook/galactica-1.3b | 267.75 |
| EleutherAI/pythia-70m | 275.0 |
| EleutherAI/pythia-160m | 205.25 |
| EleutherAI/pythia-410m | 218.5 |
| EleutherAI/pythia-1b | 167.5 |
| EleutherAI/pythia-1.4b | 157.37 |
| EleutherAI/pythia-2.8b | 116.93 |
| EleutherAI/gpt-neo-125m | 455.25 |
| EleutherAI/gpt-neo-1.3B | 475.25 |
| cerebras/Cerebras-GPT-111M | 482.75 |
| cerebras/Cerebras-GPT-256M | 424.5 |
| cerebras/Cerebras-GPT-590M | 342.5 |
| cerebras/Cerebras-GPT-1.3B | 240.875 |
| cerebras/Cerebras-GPT-2.7B | 214.25 |
| bigscience/bloom-560m | 289.5 |
| bigscience/bloom-1b1 | 158.0 |
| bigscience/bloom-1b7 | 249.5 |