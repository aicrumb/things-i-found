{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# DiscordLM\n",
        "\n",
        "Language model you can train with Notsobot's `.code` command. Alltogether this is 1362 characters out of the 2000 character limit for Discord messages. (not including the header and footer needed to send the message to the bot)\n",
        "\n",
        "This code trains a model with k input nodes, two 512 width hidden layers, and k output nodes, where k = the number of unique tokens delimited by whitespace in x (training data). Between each layer is a ReLU operation.\n",
        "```\n",
        "one hot -> hidden layer 1 -> hidden layer 2 -> logits out\n",
        "\n",
        "example visual\n",
        "\n",
        "k  768 768  k\n",
        "    o - o\n",
        "1 - o - o - 0.0010\n",
        "0 - o - o - 0.9732\n",
        ". - . - . - .\n",
        ". - . - . - .\n",
        "0 - o - o - 0.0043\n",
        "0 - o - o - 0.1224\n",
        "    o - o\n",
        "```\n",
        "\n",
        "After training for 300 epochs, we get results. After every 50 epochs we print the epoch number and the loss value."
      ],
      "metadata": {
        "id": "ipBIuU3wYYnZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# .code python ```python\n",
        "from numpy import *\n",
        "x='humpty dumpty sat on a wall humpty dumpty had a great fall all the kings horses and all the kings men couldnt put humpty together again'\n",
        "class k:\n",
        "\tdef __init__(s):s.t=list(set(x.split(' ')))\n",
        "\tdef f(s,t):\n",
        "\t\ta=[]\n",
        "\t\tfor tk in t.split(' '):b=[0]*len(s.t);b[s.t.index(tk)]=1;a.append(b)\n",
        "\t\treturn a\n",
        "\tdef d(s,t):return s.t[t]\n",
        "class M:\n",
        "\tdef __init__(A,i,h,o):\n",
        "\t\tC=i;A.i=C;A.h=h;A.o=o;A.n=len(h)+1;A.w=[];B=C\n",
        "\t\tfor D in h:A.w.append(random.randn(B,D)*.1);B=D\n",
        "\t\tA.w.append(random.randn(B,o)*.1)\n",
        "\tdef f(A,X):\n",
        "\t\tA.z=[];A.a=[X]\n",
        "\t\tfor B in range(A.n):C=dot(A.a[B],A.w[B]);A.z.append(C);D=A.r(C);A.a.append(D)\n",
        "\t\treturn A.a[-1]\n",
        "\tdef b(A,X,y,r):\n",
        "\t\tD=X.shape[0];C=[A.a[-1]-y];E=[];F=[]\n",
        "\t\tfor B in reversed(range(A.n)):E.append(dot(A.a[B].T,C[-1])/D);F.append(sum(C[-1],axis=0)/D);C.append(dot(C[-1],A.w[B].T)*A.d(A.z[B-1]))\n",
        "\t\tfor B in range(A.n):A.w[B]-=r*E[A.n-1-B]\n",
        "\tdef r(A,x):return maximum(0,x)\n",
        "\tdef d(A,x):return where(x>0,1,0)\n",
        "\tdef c(A,yp,y):return-mean(y*log(yp+1e-8)+(1-y)*log(1-yp+1e-8))\n",
        "r=.01\n",
        "ne=300\n",
        "t=k()\n",
        "l=len(t.t)\n",
        "m=M(l,[512]*2,l)\n",
        "X=array(t.f(x)[:-1])\n",
        "y=array(t.f(x)[1:])\n",
        "for e in range(ne):\n",
        "\tp=m.f(X);l=m.c(p,y);m.b(X,y,r)\n",
        "\tif(e+1)%50==0:print('{},{:.4f}'.format(e+1,l))\n",
        "\tr*=0.99\n",
        "s=' '.join(x.split(' ')[:9])\n",
        "def tk(logits,k=1,t=.6):A=logits;A/=t;B=argsort(A)[-k:];C=A[B]/sum(A[B]);D=random.choice(B,p=C);return D\n",
        "for i in range(64):s+=' '+t.d(tk(m.f(array(t.f(s)))[-1],3))\n",
        "print(s)\n",
        "# ```"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4N9U0m6TWO3e",
        "outputId": "478c8302-12f5-4658-8b66-a9bab5f1a1d2"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "50,0.2463\n",
            "100,0.1469\n",
            "150,0.1309\n",
            "200,0.1241\n",
            "250,0.1206\n",
            "300,0.1186\n",
            "humpty dumpty sat on a wall humpty dumpty had on a wall all the kings men a wall humpty dumpty sat on men couldnt put humpty dumpty sat on men a wall humpty dumpty fall a wall humpty dumpty sat on a wall all and all all the kings horses and all all the kings horses a all the kings horses and the kings men a great had a wall humpty dumpty sat\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Code explanation, written by GPT4:\n",
        "\n",
        "This code defines two classes, k and M, and uses them to generate a text sequence based on an input string. The input string is a line from the nursery rhyme \"Humpty Dumpty\".\n",
        "\n",
        "The k class is used for text preprocessing, mainly for tokenizing the input string and creating one-hot encoding for the words. The M class defines a neural network model with a given input size, hidden layers, and output size. The model is trained using backpropagation, and the output is a new sequence based on the input string.\n",
        "\n",
        "Here's a step-by-step explanation of the code:\n",
        "\n",
        "1. Import the numpy library.\n",
        "2. Define the input string 'x' as the Humpty Dumpty nursery rhyme.\n",
        "3. Define the k class, which is used for text preprocessing:\n",
        "   - The constructor (`__init__`) creates a list of unique words (tokens) in the input string.\n",
        "   - The 'f' method takes a string and returns a one-hot encoded representation of the words.\n",
        "   - The 'd' method returns the token at a given index.\n",
        "4. Define the M class, which is a neural network model:\n",
        "   - The constructor (`__init__`) initializes the model with input size, hidden layers, output size, and random weights.\n",
        "   - The 'f' method computes the forward pass of the model.\n",
        "   - The 'b' method performs backpropagation for the model, updating the weights based on the loss.\n",
        "   - The 'r' method is the ReLU (Rectified Linear Unit) activation function, which returns the maximum of 0 and the input value.\n",
        "   - The 'd' method computes the derivative of the ReLU function.\n",
        "   - The 'c' method calculates the cross-entropy loss between the predicted and true values.\n",
        "5. Set the learning rate 'r' and the number of epochs 'ne'.\n",
        "6. Create an instance of the k class 't' and get the length of the unique tokens 'l'.\n",
        "7. Create an instance of the M class 'm' with input size, hidden layers, and output size equal to the length of unique tokens.\n",
        "8. One-hot encode the input string 'x' using the k class instance 't', and create input 'X' and output 'y' arrays.\n",
        "9. Train the model 'm' for the specified number of epochs, updating the learning rate and printing the loss every 50 epochs.\n",
        "10. Define a function 'tk' to sample a token from the model's output logits using temperature and top-k sampling.\n",
        "11. Generate a new text sequence by feeding the initial words into the model and sampling the next word using the 'tk' function. Repeat this for 64 words.\n",
        "12. Print the generated text sequence."
      ],
      "metadata": {
        "id": "rplAsK8sekCy"
      }
    }
  ]
}